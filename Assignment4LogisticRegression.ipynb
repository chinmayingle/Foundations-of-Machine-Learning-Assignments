{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kzQbdaIfjLp",
        "outputId": "ef1e570e-1998-40b9-eb15-feafad7ab297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After 1 iteration: θ0 = -1.0032, θ1 = 1.5054, θ2 = 0.5020\n",
            "Converged after 83655 iterations with parameter change 0.000100.\n",
            "Final parameters: θ0 = -20.4000, θ1 = 30.1154, θ2 = 7.2970\n",
            "Training Loss: 0.0086\n",
            "Test Set Evaluation:\n",
            "Accuracy: 0.6667, Precision: 0.6000, Recall: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "class LogisticRegressionGD:\n",
        "    def __init__(self, learning_rate=0.1, tolerance=1e-6, max_iterations=60000):\n",
        "        \"\"\"\n",
        "        Initialize the Logistic Regression model with gradient descent.\n",
        "        Args:\n",
        "        - learning_rate: Step size for gradient descent.\n",
        "        - tolerance: Threshold for parameter change to stop iterations.\n",
        "        - max_iterations: Maximum number of iterations to avoid infinite loop.\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.tolerance = tolerance\n",
        "        self.max_iterations = max_iterations\n",
        "        self.theta_0 = None\n",
        "        self.theta_1 = None\n",
        "        self.theta_2 = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Sigmoid function.\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict probabilities for given input.\n",
        "        \"\"\"\n",
        "        z = self.theta_0 + self.theta_1 * X[:, 0] + self.theta_2 * X[:, 1]\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def compute_loss(self, y, y_pred):\n",
        "        \"\"\"\n",
        "        Compute the cross-entropy loss.\n",
        "        \"\"\"\n",
        "        epsilon = 1e-15  # To prevent log(0)\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
        "\n",
        "    def fit(self, x1, x2, y):\n",
        "        \"\"\"\n",
        "        Fit the logistic regression model using gradient descent.\n",
        "        Args:\n",
        "        - x1: Feature 1.\n",
        "        - x2: Feature 2.\n",
        "        - y: Target labels.\n",
        "        \"\"\"\n",
        "        # Initialize parameters\n",
        "        self.theta_0 = -1\n",
        "        self.theta_1 = 1.5\n",
        "        self.theta_2 = 0.5\n",
        "\n",
        "        N = len(y)\n",
        "        iteration = 0\n",
        "        change = np.inf  # Track change in parameters\n",
        "\n",
        "        while iteration < self.max_iterations and change > self.tolerance:\n",
        "            # Compute predictions\n",
        "            f_theta = self.theta_0 + self.theta_1 * x1 + self.theta_2 * x2\n",
        "            P = self.sigmoid(f_theta)\n",
        "\n",
        "            # Compute gradients\n",
        "            delta = P - y\n",
        "            grad_theta_0 = np.sum(delta) / N\n",
        "            grad_theta_1 = np.sum(delta * x1) / N\n",
        "            grad_theta_2 = np.sum(delta * x2) / N\n",
        "\n",
        "            # Update parameters\n",
        "            new_theta_0 = self.theta_0 - self.learning_rate * grad_theta_0\n",
        "            new_theta_1 = self.theta_1 - self.learning_rate * grad_theta_1\n",
        "            new_theta_2 = self.theta_2 - self.learning_rate * grad_theta_2\n",
        "\n",
        "            # Compute parameter change\n",
        "            change = np.sqrt(\n",
        "                (new_theta_0 - self.theta_0) ** 2\n",
        "                + (new_theta_1 - self.theta_1) ** 2\n",
        "                + (new_theta_2 - self.theta_2) ** 2\n",
        "            )\n",
        "            # print(change)\n",
        "\n",
        "            # Update parameters for next iteration\n",
        "            self.theta_0, self.theta_1, self.theta_2 = new_theta_0, new_theta_1, new_theta_2\n",
        "\n",
        "            # Print parameters after first iteration\n",
        "            if iteration == 0:\n",
        "                print(\n",
        "                    f\"After 1 iteration: θ0 = {self.theta_0:.4f}, θ1 = {self.theta_1:.4f}, θ2 = {self.theta_2:.4f}\"\n",
        "                )\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "        print(f\"Converged after {iteration} iterations with parameter change {change:.6f}.\")\n",
        "\n",
        "    def predict(self, x1, x2):\n",
        "        \"\"\"\n",
        "        Predict probabilities for the input features.\n",
        "        Args:\n",
        "        - x1: Feature 1.\n",
        "        - x2: Feature 2.\n",
        "        Returns:\n",
        "        - Probabilities of y = 1.\n",
        "        \"\"\"\n",
        "        f_theta = self.theta_0 + self.theta_1 * x1 + self.theta_2 * x2\n",
        "        return self.sigmoid(f_theta)\n",
        "\n",
        "    def get_params(self):\n",
        "        \"\"\"\n",
        "        Get the learned parameters.\n",
        "        \"\"\"\n",
        "        return self.theta_0, self.theta_1, self.theta_2\n",
        "\n",
        "    def evaluate(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Evaluate the model using accuracy, precision, and recall.\n",
        "        \"\"\"\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred)\n",
        "        recall = recall_score(y_true, y_pred)\n",
        "        return accuracy, precision, recall\n",
        "\n",
        "\n",
        "# Training data from Table 1\n",
        "train_data = np.array([\n",
        "    [0.346, 0.780, 0],\n",
        "    [0.303, 0.439, 0],\n",
        "    [0.358, 0.729, 0],\n",
        "    [0.602, 0.863, 1],\n",
        "    [0.790, 0.753, 1],\n",
        "    [0.611, 0.965, 1]\n",
        "])\n",
        "\n",
        "x1_train = train_data[:, 0]\n",
        "x2_train = train_data[:, 1]\n",
        "y_train = train_data[:, 2]\n",
        "\n",
        "# print(x1_train)\n",
        "# print(x2_train)\n",
        "# print(y_train)\n",
        "# Test data from Table 2\n",
        "test_data = np.array([\n",
        "    [0.959, 0.382, 0],\n",
        "    [0.750, 0.306, 0],\n",
        "    [0.395, 0.760, 0],\n",
        "    [0.823, 0.764, 1],\n",
        "    [0.761, 0.874, 1],\n",
        "    [0.844, 0.435, 1]\n",
        "])\n",
        "\n",
        "x1_test = test_data[:, 0]\n",
        "x2_test = test_data[:, 1]\n",
        "y_test = test_data[:, 2]\n",
        "\n",
        "# Create and train the logistic regression model\n",
        "model = LogisticRegressionGD(learning_rate=0.1, tolerance=1e-4, max_iterations=600000)\n",
        "model.fit(x1_train, x2_train, y_train)\n",
        "\n",
        "# Get learned parameters\n",
        "theta_0, theta_1, theta_2 = model.get_params()\n",
        "print(f\"Final parameters: θ0 = {theta_0:.4f}, θ1 = {theta_1:.4f}, θ2 = {theta_2:.4f}\")\n",
        "\n",
        "# Test the model on the test set\n",
        "proba_test = model.predict(x1_test, x2_test)\n",
        "predicted_classes = (proba_test >= 0.5).astype(int)\n",
        "accuracy, precision, recall = model.evaluate(y_test, predicted_classes)\n",
        "\n",
        "# Compute training loss\n",
        "loss_train = model.compute_loss(y_train, model.predict(x1_train, x2_train))\n",
        "\n",
        "print(f\"Training Loss: {loss_train:.4f}\")\n",
        "print(f\"Test Set Evaluation:\\nAccuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n"
      ]
    }
  ]
}